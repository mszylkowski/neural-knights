# Training `linear_256h_2l`

Started on `2024-04-24 17:54:53.383599`

**Description:** Add description here

Arguments:
- **`--name`**: linear_256h_2l
- **`--test`**: False
- **`--config`**: <_io.TextIOWrapper name='.\\configs\\linear.yaml' mode='r' encoding='cp1252'>
- **`--batchsize`**: 512
- **`--lr`**: 0.01
- **`--exponential_decay`**: 0.9999
- **`--reg_l2`**: 0.0005
- **`--momentum`**: 0.9
- **`--model`**: Linear
- **`--model_hidden_layers`**: 2
- **`--model_hidden_size`**: 256
- **`--criterion`**: CrossEntropyLoss()
- **`--optimizer`**: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
)

## Model

Saved in `runs/linear_256h_2l.pt`

```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Linear                                   [512, 1796]               --
├─Linear: 1-1                            [512, 256]                196,864
├─ELU: 1-2                               [512, 256]                --
├─Sequential: 1-3                        [512, 256]                --
│    └─Linear: 2-1                       [512, 256]                65,792
│    └─ELU: 2-2                          [512, 256]                --
├─Linear: 1-4                            [512, 1796]               461,572
==========================================================================================
Total params: 724,228
Trainable params: 724,228
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 370.80
==========================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 9.45
Params size (MB): 2.90
Estimated Total Size (MB): 13.92
==========================================================================================
```

## Training

| Epoch | Train/Val | Loss | Acc | Time |
| - | - | - | - | - |
| 00000 | training | 7.498 | 0.000 | ---------- |
| -------- | validation | 7.497 | 0.000 | 29 |
| 00010 | training | 6.125 | 0.040 | ---------- |
| -------- | validation | 5.858 | 0.059 | 53 |
| 00020 | training | 5.741 | 0.072 | ---------- |
| -------- | validation | 5.576 | 0.090 | 76 |
| 00030 | training | 5.512 | 0.094 | ---------- |
| -------- | validation | 5.361 | 0.106 | 99 |
| 00040 | training | 5.329 | 0.111 | ---------- |
| -------- | validation | 5.181 | 0.120 | 123 |
| 00050 | training | 5.151 | 0.126 | ---------- |
| -------- | validation | 5.062 | 0.137 | 146 |
| 00060 | training | 5.004 | 0.136 | ---------- |
| -------- | validation | 4.974 | 0.139 | 169 |
| 00070 | training | 4.874 | 0.146 | ---------- |
| -------- | validation | 4.800 | 0.155 | 193 |
| 00080 | training | 4.754 | 0.153 | ---------- |
| -------- | validation | 4.674 | 0.159 | 216 |
| 00090 | training | 4.629 | 0.162 | ---------- |
| -------- | validation | 4.529 | 0.171 | 239 |
| 00100 | training | 4.539 | 0.165 | ---------- |
| -------- | validation | 4.481 | 0.167 | 263 |
| 00110 | training | 4.443 | 0.172 | ---------- |
| -------- | validation | 4.379 | 0.183 | 286 |
| 00120 | training | 4.349 | 0.177 | ---------- |
| -------- | validation | 4.356 | 0.174 | 309 |
| 00130 | training | 4.273 | 0.180 | ---------- |
| -------- | validation | 4.296 | 0.174 | 331 |
| 00140 | training | 4.207 | 0.185 | ---------- |
| -------- | validation | 4.148 | 0.192 | 354 |
| 00150 | training | 4.133 | 0.189 | ---------- |
| -------- | validation | 4.077 | 0.188 | 378 |
| 00160 | training | 4.074 | 0.192 | ---------- |
| -------- | validation | 4.009 | 0.208 | 401 |
| 00170 | training | 4.019 | 0.195 | ---------- |
| -------- | validation | 3.963 | 0.201 | 425 |
| 00180 | training | 3.970 | 0.198 | ---------- |
| -------- | validation | 4.012 | 0.203 | 447 |
| 00190 | training | 3.923 | 0.200 | ---------- |
| -------- | validation | 3.857 | 0.202 | 471 |
| 00200 | training | 3.878 | 0.203 | ---------- |
| -------- | validation | 3.842 | 0.210 | 495 |
| 00210 | training | 3.842 | 0.206 | ---------- |
| -------- | validation | 3.933 | 0.198 | 518 |
| 00220 | training | 3.804 | 0.207 | ---------- |
| -------- | validation | 3.751 | 0.213 | 542 |
| 00230 | training | 3.769 | 0.209 | ---------- |
| -------- | validation | 3.773 | 0.212 | 566 |
| 00240 | training | 3.737 | 0.210 | ---------- |
| -------- | validation | 3.672 | 0.222 | 590 |
| 00250 | training | 3.696 | 0.213 | ---------- |
| -------- | validation | 3.626 | 0.219 | 614 |
| 00260 | training | 3.673 | 0.214 | ---------- |
| -------- | validation | 3.640 | 0.210 | 637 |
| 00270 | training | 3.645 | 0.216 | ---------- |
| -------- | validation | 3.662 | 0.209 | 660 |
| 00280 | training | 3.602 | 0.219 | ---------- |
| -------- | validation | 3.563 | 0.220 | 683 |
| 00290 | training | 3.578 | 0.221 | ---------- |
| -------- | validation | 3.567 | 0.218 | 707 |
| 00300 | training | 3.566 | 0.221 | ---------- |
| -------- | validation | 3.540 | 0.224 | 730 |
| 00310 | training | 3.546 | 0.222 | ---------- |
| -------- | validation | 3.498 | 0.229 | 753 |
| 00320 | training | 3.514 | 0.224 | ---------- |
| -------- | validation | 3.486 | 0.236 | 777 |
| 00330 | training | 3.497 | 0.224 | ---------- |
| -------- | validation | 3.502 | 0.222 | 802 |
| 00340 | training | 3.479 | 0.225 | ---------- |
| -------- | validation | 3.459 | 0.223 | 828 |
| 00350 | training | 3.454 | 0.227 | ---------- |
| -------- | validation | 3.453 | 0.223 | 853 |
| 00360 | training | 3.434 | 0.229 | ---------- |
| -------- | validation | 3.478 | 0.222 | 878 |
| 00370 | training | 3.428 | 0.228 | ---------- |
| -------- | validation | 3.371 | 0.235 | 903 |
| 00380 | training | 3.414 | 0.229 | ---------- |
| -------- | validation | 3.428 | 0.227 | 928 |
| 00390 | training | 3.399 | 0.231 | ---------- |
| -------- | validation | 3.439 | 0.230 | 953 |
| 00400 | training | 3.395 | 0.229 | ---------- |
| -------- | validation | 3.347 | 0.225 | 977 |
| 00410 | training | 3.373 | 0.231 | ---------- |
| -------- | validation | 3.378 | 0.229 | 1001 |
| 00420 | training | 3.365 | 0.233 | ---------- |
| -------- | validation | 3.336 | 0.236 | 1023 |
| 00430 | training | 3.353 | 0.232 | ---------- |
| -------- | validation | 3.349 | 0.234 | 1049 |
| 00440 | training | 3.339 | 0.233 | ---------- |
| -------- | validation | 3.355 | 0.221 | 1072 |
| 00450 | training | 3.329 | 0.234 | ---------- |
| -------- | validation | 3.266 | 0.244 | 1096 |
| 00460 | training | 3.316 | 0.234 | ---------- |
| -------- | validation | 3.291 | 0.235 | 1119 |
| 00470 | training | 3.305 | 0.234 | ---------- |
| -------- | validation | 3.250 | 0.240 | 1142 |
| 00480 | training | 3.302 | 0.235 | ---------- |
| -------- | validation | 3.288 | 0.224 | 1166 |
| 00490 | training | 3.291 | 0.235 | ---------- |
| -------- | validation | 3.277 | 0.242 | 1188 |
| 00500 | training | 3.283 | 0.236 | ---------- |
| -------- | validation | 3.264 | 0.232 | 1212 |
| 00510 | training | 3.266 | 0.237 | ---------- |
| -------- | validation | 3.201 | 0.239 | 1234 |
| 00520 | training | 3.260 | 0.238 | ---------- |
| -------- | validation | 3.236 | 0.237 | 1257 |
| 00530 | training | 3.251 | 0.238 | ---------- |
| -------- | validation | 3.223 | 0.244 | 1280 |
| 00540 | training | 3.246 | 0.238 | ---------- |
| -------- | validation | 3.269 | 0.224 | 1303 |
| 00550 | training | 3.244 | 0.238 | ---------- |
| -------- | validation | 3.185 | 0.248 | 1326 |
| 00560 | training | 3.237 | 0.239 | ---------- |
| -------- | validation | 3.248 | 0.236 | 1349 |
| 00570 | training | 3.232 | 0.238 | ---------- |
| -------- | validation | 3.178 | 0.248 | 1372 |
| 00580 | training | 3.215 | 0.240 | ---------- |
| -------- | validation | 3.165 | 0.250 | 1396 |
| 00590 | training | 3.209 | 0.241 | ---------- |
| -------- | validation | 3.196 | 0.247 | 1419 |
| 00600 | training | 3.207 | 0.241 | ---------- |
| -------- | validation | 3.173 | 0.243 | 1442 |
| 00610 | training | 3.203 | 0.241 | ---------- |
| -------- | validation | 3.211 | 0.237 | 1467 |
| 00620 | training | 3.199 | 0.241 | ---------- |
| -------- | validation | 3.150 | 0.249 | 1490 |
| 00630 | training | 3.195 | 0.241 | ---------- |
| -------- | validation | 3.166 | 0.247 | 1514 |
| 00640 | training | 3.184 | 0.243 | ---------- |
| -------- | validation | 3.155 | 0.250 | 1537 |
| 00650 | training | 3.184 | 0.241 | ---------- |
| -------- | validation | 3.189 | 0.249 | 1560 |
| 00660 | training | 3.179 | 0.243 | ---------- |
| -------- | validation | 3.187 | 0.243 | 1584 |
| 00670 | training | 3.173 | 0.243 | ---------- |
| -------- | validation | 3.208 | 0.234 | 1608 |
| 00680 | training | 3.175 | 0.242 | ---------- |
| -------- | validation | 3.160 | 0.239 | 1631 |
| 00690 | training | 3.165 | 0.244 | ---------- |
| -------- | validation | 3.177 | 0.246 | 1654 |
| 00700 | training | 3.152 | 0.245 | ---------- |
| -------- | validation | 3.132 | 0.246 | 1678 |
| 00710 | training | 3.159 | 0.245 | ---------- |
| -------- | validation | 3.210 | 0.230 | 1701 |
| 00720 | training | 3.158 | 0.244 | ---------- |
| -------- | validation | 3.137 | 0.245 | 1725 |
| 00730 | training | 3.154 | 0.243 | ---------- |
| -------- | validation | 3.134 | 0.249 | 1748 |
| 00740 | training | 3.147 | 0.244 | ---------- |
| -------- | validation | 3.154 | 0.239 | 1773 |
| 00750 | training | 3.144 | 0.245 | ---------- |
| -------- | validation | 3.148 | 0.251 | 1797 |
| 00760 | training | 3.144 | 0.244 | ---------- |
| -------- | validation | 3.220 | 0.227 | 1821 |
| 00770 | training | 3.137 | 0.245 | ---------- |
| -------- | validation | 3.118 | 0.252 | 1845 |
| 00780 | training | 3.141 | 0.244 | ---------- |
| -------- | validation | 3.150 | 0.252 | 1869 |
| 00790 | training | 3.124 | 0.246 | ---------- |
| -------- | validation | 3.094 | 0.261 | 1893 |
| 00800 | training | 3.131 | 0.246 | ---------- |
| -------- | validation | 3.166 | 0.237 | 1917 |
| 00810 | training | 3.127 | 0.246 | ---------- |
| -------- | validation | 3.138 | 0.237 | 1941 |
| 00820 | training | 3.128 | 0.246 | ---------- |
| -------- | validation | 3.103 | 0.244 | 1965 |
| 00830 | training | 3.126 | 0.246 | ---------- |
| -------- | validation | 3.149 | 0.235 | 1988 |
| 00840 | training | 3.121 | 0.246 | ---------- |
| -------- | validation | 3.048 | 0.250 | 2012 |
| 00850 | training | 3.117 | 0.246 | ---------- |
| -------- | validation | 3.115 | 0.249 | 2035 |
| 00860 | training | 3.118 | 0.247 | ---------- |
| -------- | validation | 3.116 | 0.248 | 2061 |
| 00870 | training | 3.109 | 0.246 | ---------- |
| -------- | validation | 3.113 | 0.244 | 2085 |
| 00880 | training | 3.106 | 0.246 | ---------- |
| -------- | validation | 3.108 | 0.252 | 2109 |
| 00890 | training | 3.106 | 0.248 | ---------- |
| -------- | validation | 3.101 | 0.245 | 2132 |
| 00900 | training | 3.114 | 0.246 | ---------- |
| -------- | validation | 3.085 | 0.247 | 2155 |
| 00910 | training | 3.111 | 0.246 | ---------- |
| -------- | validation | 3.077 | 0.256 | 2179 |
| 00920 | training | 3.098 | 0.248 | ---------- |
| -------- | validation | 3.080 | 0.251 | 2203 |
| 00930 | training | 3.100 | 0.246 | ---------- |
| -------- | validation | 3.122 | 0.247 | 2226 |
| 00940 | training | 3.096 | 0.247 | ---------- |
| -------- | validation | 3.130 | 0.240 | 2248 |
| 00950 | training | 3.098 | 0.246 | ---------- |
| -------- | validation | 3.091 | 0.239 | 2272 |
| 00960 | training | 3.091 | 0.247 | ---------- |
| -------- | validation | 3.104 | 0.246 | 2295 |
| 00970 | training | 3.103 | 0.245 | ---------- |
| -------- | validation | 3.097 | 0.247 | 2318 |
| 00980 | training | 3.087 | 0.249 | ---------- |
| -------- | validation | 3.115 | 0.238 | 2340 |
