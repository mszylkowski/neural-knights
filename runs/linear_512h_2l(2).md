# Training `linear_512h_2l(2)`

Started on `2024-04-24 14:25:45.474964`

**Description:** Add description here

Arguments:
- **`--name`**: linear_512h_2l(2)
- **`--test`**: False
- **`--config`**: <_io.TextIOWrapper name='.\\configs\\linear.yaml' mode='r' encoding='cp1252'>
- **`--batchsize`**: 512
- **`--lr`**: 0.01
- **`--exponential_decay`**: 0.9999
- **`--reg_l2`**: 0.0005
- **`--momentum`**: 0.9
- **`--model`**: Linear
- **`--model_hidden_layers`**: 2
- **`--model_hidden_size`**: 512
- **`--criterion`**: CrossEntropyLoss()
- **`--optimizer`**: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
)

## Model

Saved in `runs/linear_512h_2l(2).pt`

```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Linear                                   [512, 1796]               --
├─Linear: 1-1                            [512, 512]                393,728
├─ELU: 1-2                               [512, 512]                --
├─Sequential: 1-3                        [512, 512]                --
│    └─Linear: 2-1                       [512, 512]                262,656
│    └─ELU: 2-2                          [512, 512]                --
├─Linear: 1-4                            [512, 1796]               921,348
==========================================================================================
Total params: 1,577,732
Trainable params: 1,577,732
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 807.80
==========================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 11.55
Params size (MB): 6.31
Estimated Total Size (MB): 19.43
==========================================================================================
```

## Training

| Epoch | Train/Val | Loss | Acc | Time |
| - | - | - | - | - |
| 00000 | training | 7.491 | 0.000 | ---------- |
| -------- | validation | 7.491 | 0.001 | 33 |
| 00010 | training | 6.089 | 0.045 | ---------- |
| -------- | validation | 5.757 | 0.069 | 59 |
| 00020 | training | 5.692 | 0.078 | ---------- |
| -------- | validation | 5.505 | 0.097 | 85 |
| 00030 | training | 5.440 | 0.101 | ---------- |
| -------- | validation | 5.278 | 0.114 | 112 |
| 00040 | training | 5.230 | 0.121 | ---------- |
| -------- | validation | 5.049 | 0.138 | 138 |
| 00050 | training | 5.043 | 0.135 | ---------- |
| -------- | validation | 4.907 | 0.151 | 164 |
| 00060 | training | 4.898 | 0.144 | ---------- |
| -------- | validation | 4.857 | 0.147 | 190 |
| 00070 | training | 4.767 | 0.154 | ---------- |
| -------- | validation | 4.665 | 0.158 | 215 |
| 00080 | training | 4.644 | 0.162 | ---------- |
| -------- | validation | 4.523 | 0.173 | 240 |
| 00090 | training | 4.517 | 0.169 | ---------- |
| -------- | validation | 4.492 | 0.167 | 265 |
| 00100 | training | 4.428 | 0.173 | ---------- |
| -------- | validation | 4.272 | 0.184 | 290 |
| 00110 | training | 4.333 | 0.180 | ---------- |
| -------- | validation | 4.332 | 0.180 | 316 |
| 00120 | training | 4.240 | 0.184 | ---------- |
| -------- | validation | 4.261 | 0.177 | 341 |
| 00130 | training | 4.167 | 0.188 | ---------- |
| -------- | validation | 4.145 | 0.192 | 365 |
| 00140 | training | 4.101 | 0.193 | ---------- |
| -------- | validation | 4.084 | 0.187 | 390 |
| 00150 | training | 4.029 | 0.197 | ---------- |
| -------- | validation | 4.000 | 0.196 | 415 |
| 00160 | training | 3.972 | 0.200 | ---------- |
| -------- | validation | 3.875 | 0.206 | 440 |
| 00170 | training | 3.916 | 0.204 | ---------- |
| -------- | validation | 3.889 | 0.209 | 465 |
| 00180 | training | 3.866 | 0.207 | ---------- |
| -------- | validation | 3.851 | 0.208 | 489 |
| 00190 | training | 3.822 | 0.208 | ---------- |
| -------- | validation | 3.792 | 0.213 | 514 |
| 00200 | training | 3.778 | 0.211 | ---------- |
| -------- | validation | 3.730 | 0.220 | 538 |
| 00210 | training | 3.745 | 0.213 | ---------- |
| -------- | validation | 3.765 | 0.205 | 563 |
| 00220 | training | 3.708 | 0.215 | ---------- |
| -------- | validation | 3.700 | 0.220 | 588 |
| 00230 | training | 3.676 | 0.217 | ---------- |
| -------- | validation | 3.659 | 0.217 | 613 |
| 00240 | training | 3.644 | 0.218 | ---------- |
| -------- | validation | 3.610 | 0.224 | 638 |
| 00250 | training | 3.606 | 0.221 | ---------- |
| -------- | validation | 3.602 | 0.221 | 663 |
| 00260 | training | 3.586 | 0.222 | ---------- |
| -------- | validation | 3.604 | 0.220 | 688 |
| 00270 | training | 3.559 | 0.223 | ---------- |
| -------- | validation | 3.539 | 0.218 | 713 |
| 00280 | training | 3.519 | 0.226 | ---------- |
| -------- | validation | 3.556 | 0.211 | 737 |
| 00290 | training | 3.495 | 0.228 | ---------- |
| -------- | validation | 3.492 | 0.228 | 762 |
| 00300 | training | 3.485 | 0.227 | ---------- |
| -------- | validation | 3.436 | 0.235 | 788 |
| 00310 | training | 3.468 | 0.229 | ---------- |
| -------- | validation | 3.508 | 0.215 | 812 |
| 00320 | training | 3.439 | 0.230 | ---------- |
| -------- | validation | 3.364 | 0.243 | 836 |
| 00330 | training | 3.421 | 0.232 | ---------- |
| -------- | validation | 3.385 | 0.233 | 861 |
| 00340 | training | 3.403 | 0.232 | ---------- |
| -------- | validation | 3.361 | 0.231 | 886 |
| 00350 | training | 3.381 | 0.234 | ---------- |
| -------- | validation | 3.396 | 0.236 | 905 |
| 00360 | training | 3.364 | 0.235 | ---------- |
| -------- | validation | 3.373 | 0.230 | 923 |
| 00370 | training | 3.357 | 0.234 | ---------- |
| -------- | validation | 3.291 | 0.247 | 942 |
| 00380 | training | 3.344 | 0.235 | ---------- |
| -------- | validation | 3.320 | 0.239 | 970 |
| 00390 | training | 3.331 | 0.237 | ---------- |
| -------- | validation | 3.318 | 0.240 | 992 |
| 00400 | training | 3.327 | 0.235 | ---------- |
| -------- | validation | 3.248 | 0.246 | 1014 |
| 00410 | training | 3.305 | 0.238 | ---------- |
| -------- | validation | 3.314 | 0.239 | 1037 |
| 00420 | training | 3.298 | 0.239 | ---------- |
| -------- | validation | 3.241 | 0.254 | 1059 |
| 00430 | training | 3.287 | 0.239 | ---------- |
| -------- | validation | 3.297 | 0.226 | 1081 |
| 00440 | training | 3.273 | 0.239 | ---------- |
| -------- | validation | 3.278 | 0.233 | 1104 |
| 00450 | training | 3.263 | 0.240 | ---------- |
| -------- | validation | 3.248 | 0.234 | 1126 |
| 00460 | training | 3.249 | 0.240 | ---------- |
| -------- | validation | 3.256 | 0.239 | 1149 |
| 00470 | training | 3.239 | 0.242 | ---------- |
| -------- | validation | 3.202 | 0.237 | 1171 |
| 00480 | training | 3.235 | 0.241 | ---------- |
| -------- | validation | 3.207 | 0.239 | 1195 |
| 00490 | training | 3.226 | 0.242 | ---------- |
| -------- | validation | 3.190 | 0.241 | 1219 |
| 00500 | training | 3.218 | 0.243 | ---------- |
| -------- | validation | 3.238 | 0.229 | 1241 |
| 00510 | training | 3.201 | 0.244 | ---------- |
| -------- | validation | 3.205 | 0.240 | 1264 |
| 00520 | training | 3.195 | 0.244 | ---------- |
| -------- | validation | 3.187 | 0.238 | 1286 |
| 00530 | training | 3.187 | 0.245 | ---------- |
| -------- | validation | 3.183 | 0.254 | 1309 |
| 00540 | training | 3.182 | 0.245 | ---------- |
| -------- | validation | 3.141 | 0.254 | 1332 |
| 00550 | training | 3.179 | 0.244 | ---------- |
| -------- | validation | 3.157 | 0.245 | 1355 |
| 00560 | training | 3.173 | 0.245 | ---------- |
| -------- | validation | 3.183 | 0.244 | 1378 |
| 00570 | training | 3.169 | 0.245 | ---------- |
| -------- | validation | 3.143 | 0.251 | 1401 |
| 00580 | training | 3.150 | 0.247 | ---------- |
| -------- | validation | 3.157 | 0.241 | 1423 |
| 00590 | training | 3.145 | 0.248 | ---------- |
| -------- | validation | 3.121 | 0.254 | 1446 |
| 00600 | training | 3.143 | 0.247 | ---------- |
| -------- | validation | 3.092 | 0.247 | 1469 |
| 00610 | training | 3.139 | 0.248 | ---------- |
| -------- | validation | 3.220 | 0.238 | 1492 |
| 00620 | training | 3.136 | 0.248 | ---------- |
| -------- | validation | 3.100 | 0.251 | 1516 |
| 00630 | training | 3.133 | 0.247 | ---------- |
| -------- | validation | 3.058 | 0.260 | 1540 |
| 00640 | training | 3.121 | 0.249 | ---------- |
| -------- | validation | 3.095 | 0.251 | 1564 |
| 00650 | training | 3.121 | 0.248 | ---------- |
| -------- | validation | 3.065 | 0.266 | 1588 |
| 00660 | training | 3.115 | 0.251 | ---------- |
| -------- | validation | 3.148 | 0.240 | 1611 |
| 00670 | training | 3.110 | 0.250 | ---------- |
| -------- | validation | 3.128 | 0.245 | 1635 |
| 00680 | training | 3.114 | 0.250 | ---------- |
| -------- | validation | 3.107 | 0.249 | 1659 |
| 00690 | training | 3.102 | 0.250 | ---------- |
| -------- | validation | 3.132 | 0.247 | 1683 |
| 00700 | training | 3.091 | 0.252 | ---------- |
| -------- | validation | 3.064 | 0.263 | 1706 |
| 00710 | training | 3.096 | 0.251 | ---------- |
| -------- | validation | 3.138 | 0.245 | 1730 |
| 00720 | training | 3.097 | 0.251 | ---------- |
| -------- | validation | 3.101 | 0.246 | 1753 |
| 00730 | training | 3.093 | 0.250 | ---------- |
| -------- | validation | 3.045 | 0.248 | 1777 |
| 00740 | training | 3.086 | 0.251 | ---------- |
| -------- | validation | 3.069 | 0.242 | 1801 |
| 00750 | training | 3.087 | 0.250 | ---------- |
| -------- | validation | 3.078 | 0.258 | 1825 |
| 00760 | training | 3.084 | 0.252 | ---------- |
| -------- | validation | 3.036 | 0.256 | 1849 |
| 00770 | training | 3.077 | 0.251 | ---------- |
| -------- | validation | 3.078 | 0.261 | 1873 |
| 00780 | training | 3.082 | 0.251 | ---------- |
| -------- | validation | 3.051 | 0.255 | 1899 |
| 00790 | training | 3.065 | 0.252 | ---------- |
| -------- | validation | 3.057 | 0.253 | 1923 |
| 00800 | training | 3.071 | 0.252 | ---------- |
| -------- | validation | 3.079 | 0.251 | 1947 |
| 00810 | training | 3.069 | 0.253 | ---------- |
| -------- | validation | 3.026 | 0.259 | 1972 |
| 00820 | training | 3.071 | 0.252 | ---------- |
| -------- | validation | 3.090 | 0.251 | 1998 |
| 00830 | training | 3.068 | 0.252 | ---------- |
| -------- | validation | 3.074 | 0.240 | 2022 |
| 00840 | training | 3.063 | 0.252 | ---------- |
| -------- | validation | 3.059 | 0.257 | 2047 |
| 00850 | training | 3.059 | 0.253 | ---------- |
| -------- | validation | 3.091 | 0.247 | 2071 |
| 00860 | training | 3.062 | 0.253 | ---------- |
| -------- | validation | 3.094 | 0.241 | 2094 |
| 00870 | training | 3.051 | 0.253 | ---------- |
| -------- | validation | 3.043 | 0.255 | 2118 |
| 00880 | training | 3.049 | 0.253 | ---------- |
| -------- | validation | 3.041 | 0.250 | 2142 |
| 00890 | training | 3.050 | 0.254 | ---------- |
| -------- | validation | 3.050 | 0.255 | 2166 |
| 00900 | training | 3.057 | 0.252 | ---------- |
| -------- | validation | 3.027 | 0.259 | 2190 |
| 00910 | training | 3.056 | 0.252 | ---------- |
| -------- | validation | 3.036 | 0.242 | 2215 |
| 00920 | training | 3.041 | 0.254 | ---------- |
| -------- | validation | 3.017 | 0.268 | 2239 |
| 00930 | training | 3.045 | 0.252 | ---------- |
| -------- | validation | 3.053 | 0.244 | 2262 |
| 00940 | training | 3.040 | 0.254 | ---------- |
| -------- | validation | 3.049 | 0.250 | 2287 |
| 00950 | training | 3.043 | 0.253 | ---------- |
| -------- | validation | 2.988 | 0.262 | 2311 |
| 00960 | training | 3.037 | 0.254 | ---------- |
| -------- | validation | 3.022 | 0.253 | 2335 |
| 00970 | training | 3.047 | 0.253 | ---------- |
| -------- | validation | 3.012 | 0.248 | 2358 |
| 00980 | training | 3.033 | 0.255 | ---------- |
| -------- | validation | 3.013 | 0.261 | 2382 |
| 00990 | training | 3.029 | 0.256 | ---------- |
| -------- | validation | 3.007 | 0.260 | 2406 |
| 01000 | training | 3.032 | 0.255 | ---------- |
| -------- | validation | 3.049 | 0.251 | 2430 |
| 01010 | training | 3.043 | 0.253 | ---------- |
| -------- | validation | 3.026 | 0.249 | 2453 |
| 01020 | training | 3.034 | 0.254 | ---------- |
| -------- | validation | 3.060 | 0.252 | 2477 |
| 01030 | training | 3.025 | 0.255 | ---------- |
| -------- | validation | 3.011 | 0.259 | 2501 |
| 01040 | training | 3.024 | 0.256 | ---------- |
| -------- | validation | 3.025 | 0.257 | 2524 |
| 01050 | training | 3.027 | 0.254 | ---------- |
| -------- | validation | 3.006 | 0.252 | 2549 |
| 01060 | training | 3.025 | 0.255 | ---------- |
| -------- | validation | 2.969 | 0.267 | 2572 |
| 01070 | training | 3.011 | 0.257 | ---------- |
| -------- | validation | 3.094 | 0.247 | 2596 |
| 01080 | training | 3.007 | 0.257 | ---------- |
| -------- | validation | 3.003 | 0.262 | 2619 |
