# Training `resnet_6b_256f`

Started on `2024-04-26 13:48:23.516681`

**Description:** Add description here

Arguments:
- **`--name`**: resnet_6b_256f
- **`--test`**: False
- **`--config`**: <_io.TextIOWrapper name='.\\configs\\resnet.yaml' mode='r' encoding='cp1252'>
- **`--batchsize`**: 512
- **`--lr`**: 0.000691
- **`--exponential_decay`**: 0.9999
- **`--reg_l2`**: 0.0005
- **`--momentum`**: 0.9
- **`--model`**: ResNet
- **`--model_blocks`**: 6
- **`--model_num_filters`**: 256
- **`--criterion`**: CrossEntropyLoss()
- **`--optimizer`**: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.000691
    lr: 0.000691
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
)

## Model

Saved in `runs/resnet_6b_256f.pt`

```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ResNet                                   [512, 1796]               --
├─Conv2d: 1-1                            [512, 256, 8, 8]          27,904
├─Sequential: 1-2                        [512, 256, 8, 8]          --
│    └─SimpleSkipLayer: 2-1              [512, 256, 8, 8]          1,024
│    │    └─Conv2d: 3-1                  [512, 256, 8, 8]          590,080
│    │    └─Conv2d: 3-2                  [512, 256, 8, 8]          590,080
│    └─SimpleSkipLayer: 2-2              [512, 256, 8, 8]          1,024
│    │    └─Conv2d: 3-3                  [512, 256, 8, 8]          590,080
│    │    └─Conv2d: 3-4                  [512, 256, 8, 8]          590,080
│    └─SimpleSkipLayer: 2-3              [512, 256, 8, 8]          1,024
│    │    └─Conv2d: 3-5                  [512, 256, 8, 8]          590,080
│    │    └─Conv2d: 3-6                  [512, 256, 8, 8]          590,080
│    └─SimpleSkipLayer: 2-4              [512, 256, 8, 8]          1,024
│    │    └─Conv2d: 3-7                  [512, 256, 8, 8]          590,080
│    │    └─Conv2d: 3-8                  [512, 256, 8, 8]          590,080
│    └─SimpleSkipLayer: 2-5              [512, 256, 8, 8]          1,024
│    │    └─Conv2d: 3-9                  [512, 256, 8, 8]          590,080
│    │    └─Conv2d: 3-10                 [512, 256, 8, 8]          590,080
│    └─SimpleSkipLayer: 2-6              [512, 256, 8, 8]          1,024
│    │    └─Conv2d: 3-11                 [512, 256, 8, 8]          590,080
│    │    └─Conv2d: 3-12                 [512, 256, 8, 8]          590,080
├─Linear: 1-3                            [512, 1796]               29,427,460
==========================================================================================
Total params: 36,542,468
Trainable params: 36,542,468
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 248.01
==========================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 879.77
Params size (MB): 146.15
Estimated Total Size (MB): 1027.49
==========================================================================================
```

## Training

| Epoch | Train/Val | Loss | Acc | Time |
| - | - | - | - | - |
| 03810 | training | 1.729 | 0.443 | ---------- |
| -------- | validation | 1.659 | 0.477 | 20 |
| 03820 | training | 1.694 | 0.472 | ---------- |
| -------- | validation | 1.701 | 0.471 | 156 |
| 03830 | training | 1.693 | 0.472 | ---------- |
| -------- | validation | 1.664 | 0.479 | 292 |
| 03840 | training | 1.688 | 0.473 | ---------- |
| -------- | validation | 1.711 | 0.469 | 427 |
| 03850 | training | 1.694 | 0.471 | ---------- |
| -------- | validation | 1.706 | 0.469 | 562 |
| 03860 | training | 1.687 | 0.473 | ---------- |
| -------- | validation | 1.713 | 0.464 | 698 |
| 03870 | training | 1.691 | 0.472 | ---------- |
| -------- | validation | 1.731 | 0.458 | 833 |
| 03880 | training | 1.693 | 0.472 | ---------- |
| -------- | validation | 1.685 | 0.467 | 968 |
| 03890 | training | 1.689 | 0.473 | ---------- |
| -------- | validation | 1.694 | 0.470 | 1101 |
| 03900 | training | 1.695 | 0.471 | ---------- |
| -------- | validation | 1.689 | 0.470 | 1234 |
| 03910 | training | 1.694 | 0.471 | ---------- |
| -------- | validation | 1.683 | 0.464 | 1370 |
| 03920 | training | 1.694 | 0.472 | ---------- |
| -------- | validation | 1.723 | 0.472 | 1506 |
| 03930 | training | 1.701 | 0.469 | ---------- |
| -------- | validation | 1.710 | 0.464 | 1641 |
| 03940 | training | 1.704 | 0.470 | ---------- |
| -------- | validation | 1.709 | 0.469 | 1776 |
| 03950 | training | 1.699 | 0.471 | ---------- |
| -------- | validation | 1.703 | 0.470 | 1910 |
| 03960 | training | 1.696 | 0.471 | ---------- |
| -------- | validation | 1.713 | 0.466 | 2045 |
| 03970 | training | 1.700 | 0.470 | ---------- |
| -------- | validation | 1.705 | 0.474 | 2179 |
| 03980 | training | 1.692 | 0.472 | ---------- |
| -------- | validation | 1.714 | 0.460 | 2314 |
| 03990 | training | 1.696 | 0.471 | ---------- |
| -------- | validation | 1.723 | 0.467 | 2448 |
| 04000 | training | 1.700 | 0.470 | ---------- |
| -------- | validation | 1.700 | 0.470 | 2582 |
| 04010 | training | 1.692 | 0.472 | ---------- |
| -------- | validation | 1.730 | 0.464 | 2716 |
| 04020 | training | 1.697 | 0.472 | ---------- |
| -------- | validation | 1.735 | 0.462 | 2850 |
| 04030 | training | 1.694 | 0.472 | ---------- |
| -------- | validation | 1.719 | 0.464 | 2984 |
| 04040 | training | 1.695 | 0.471 | ---------- |
| -------- | validation | 1.711 | 0.473 | 3118 |
| 04050 | training | 1.694 | 0.471 | ---------- |
| -------- | validation | 1.693 | 0.476 | 3254 |
| 04060 | training | 1.698 | 0.470 | ---------- |
| -------- | validation | 1.704 | 0.463 | 3390 |
| 04070 | training | 1.697 | 0.471 | ---------- |
| -------- | validation | 1.710 | 0.467 | 3525 |
| 04080 | training | 1.701 | 0.469 | ---------- |
| -------- | validation | 1.705 | 0.465 | 3659 |
| 04090 | training | 1.696 | 0.472 | ---------- |
| -------- | validation | 1.738 | 0.458 | 3793 |
| 04100 | training | 1.694 | 0.471 | ---------- |
| -------- | validation | 1.705 | 0.466 | 3928 |
| 04110 | training | 1.697 | 0.471 | ---------- |
| -------- | validation | 1.671 | 0.473 | 4063 |
| 04120 | training | 1.698 | 0.471 | ---------- |
| -------- | validation | 1.710 | 0.467 | 4198 |
| 04130 | training | 1.698 | 0.471 | ---------- |
| -------- | validation | 1.664 | 0.475 | 4332 |
| 04140 | training | 1.694 | 0.472 | ---------- |
| -------- | validation | 1.665 | 0.478 | 4466 |
| 04150 | training | 1.693 | 0.472 | ---------- |
| -------- | validation | 1.716 | 0.467 | 4600 |
| 04160 | training | 1.699 | 0.471 | ---------- |
| -------- | validation | 1.711 | 0.467 | 4734 |
| 04170 | training | 1.689 | 0.472 | ---------- |
| -------- | validation | 1.692 | 0.475 | 4868 |
| 04180 | training | 1.687 | 0.473 | ---------- |
| -------- | validation | 1.688 | 0.475 | 5002 |
| 04190 | training | 1.694 | 0.471 | ---------- |
| -------- | validation | 1.718 | 0.471 | 5137 |
| 04200 | training | 1.688 | 0.474 | ---------- |
| -------- | validation | 1.706 | 0.464 | 5271 |
| 04210 | training | 1.692 | 0.472 | ---------- |
| -------- | validation | 1.704 | 0.464 | 5404 |
| 04220 | training | 1.686 | 0.474 | ---------- |
| -------- | validation | 1.701 | 0.472 | 5539 |
| 04230 | training | 1.690 | 0.474 | ---------- |
| -------- | validation | 1.645 | 0.485 | 5673 |
| 04240 | training | 1.686 | 0.474 | ---------- |
| -------- | validation | 1.678 | 0.475 | 5807 |
| 04250 | training | 1.687 | 0.472 | ---------- |
| -------- | validation | 1.697 | 0.466 | 5941 |
| 04260 | training | 1.682 | 0.474 | ---------- |
| -------- | validation | 1.682 | 0.469 | 6075 |
| 04270 | training | 1.685 | 0.474 | ---------- |
| -------- | validation | 1.679 | 0.474 | 6209 |
| 04280 | training | 1.684 | 0.473 | ---------- |
| -------- | validation | 1.679 | 0.472 | 6343 |
| 04290 | training | 1.681 | 0.475 | ---------- |
| -------- | validation | 1.701 | 0.477 | 6478 |
| 04300 | training | 1.682 | 0.474 | ---------- |
| -------- | validation | 1.686 | 0.471 | 6612 |
| 04310 | training | 1.680 | 0.474 | ---------- |
| -------- | validation | 1.735 | 0.463 | 6746 |
| 04320 | training | 1.678 | 0.476 | ---------- |
| -------- | validation | 1.656 | 0.484 | 6880 |
| 04330 | training | 1.671 | 0.477 | ---------- |
| -------- | validation | 1.665 | 0.475 | 7014 |
| 04340 | training | 1.676 | 0.477 | ---------- |
| -------- | validation | 1.643 | 0.490 | 7149 |
| 04350 | training | 1.677 | 0.477 | ---------- |
| -------- | validation | 1.648 | 0.485 | 7284 |
| 04360 | training | 1.681 | 0.475 | ---------- |
| -------- | validation | 1.690 | 0.470 | 7414 |
| 04370 | training | 1.676 | 0.476 | ---------- |
| -------- | validation | 1.677 | 0.477 | 7548 |
| 04380 | training | 1.679 | 0.475 | ---------- |
| -------- | validation | 1.695 | 0.474 | 7678 |
| 04390 | training | 1.676 | 0.476 | ---------- |
| -------- | validation | 1.649 | 0.492 | 7808 |
| 04400 | training | 1.671 | 0.477 | ---------- |
| -------- | validation | 1.694 | 0.458 | 7941 |
| 04410 | training | 1.681 | 0.475 | ---------- |
| -------- | validation | 1.685 | 0.470 | 8074 |
| 04420 | training | 1.673 | 0.477 | ---------- |
| -------- | validation | 1.703 | 0.462 | 8208 |
| 04430 | training | 1.677 | 0.476 | ---------- |
| -------- | validation | 1.660 | 0.483 | 8339 |
| 04440 | training | 1.674 | 0.476 | ---------- |
| -------- | validation | 1.706 | 0.467 | 8469 |
| 04450 | training | 1.674 | 0.475 | ---------- |
| -------- | validation | 1.700 | 0.479 | 8599 |
| 04460 | training | 1.678 | 0.475 | ---------- |
| -------- | validation | 1.657 | 0.481 | 8731 |
| 04470 | training | 1.672 | 0.477 | ---------- |
| -------- | validation | 1.689 | 0.476 | 8861 |
| 04480 | training | 1.671 | 0.477 | ---------- |
| -------- | validation | 1.698 | 0.478 | 8993 |
| 04490 | training | 1.674 | 0.476 | ---------- |
| -------- | validation | 1.695 | 0.469 | 9128 |
| 04500 | training | 1.676 | 0.476 | ---------- |
| -------- | validation | 1.704 | 0.466 | 9257 |
| 04510 | training | 1.669 | 0.479 | ---------- |
| -------- | validation | 1.696 | 0.468 | 9385 |
| 04520 | training | 1.672 | 0.477 | ---------- |
| -------- | validation | 1.662 | 0.482 | 9517 |
| 04530 | training | 1.676 | 0.477 | ---------- |
| -------- | validation | 1.694 | 0.465 | 9647 |
| 04540 | training | 1.676 | 0.476 | ---------- |
| -------- | validation | 1.696 | 0.464 | 9778 |
| 04550 | training | 1.668 | 0.477 | ---------- |
| -------- | validation | 1.688 | 0.477 | 9910 |
| 04560 | training | 1.672 | 0.477 | ---------- |
| -------- | validation | 1.682 | 0.476 | 10040 |
| 04570 | training | 1.674 | 0.477 | ---------- |
| -------- | validation | 1.680 | 0.471 | 10173 |
| 04580 | training | 1.677 | 0.475 | ---------- |
| -------- | validation | 1.669 | 0.476 | 10306 |
| 04590 | training | 1.677 | 0.475 | ---------- |
| -------- | validation | 1.679 | 0.486 | 10440 |
| 04600 | training | 1.671 | 0.478 | ---------- |
| -------- | validation | 1.673 | 0.470 | 10573 |
| 04610 | training | 1.672 | 0.477 | ---------- |
| -------- | validation | 1.672 | 0.488 | 10706 |
| 04620 | training | 1.671 | 0.477 | ---------- |
| -------- | validation | 1.670 | 0.468 | 10836 |
| 04630 | training | 1.671 | 0.477 | ---------- |
| -------- | validation | 1.675 | 0.481 | 10965 |
| 04640 | training | 1.680 | 0.475 | ---------- |
| -------- | validation | 1.669 | 0.474 | 11093 |
| 04650 | training | 1.669 | 0.478 | ---------- |
| -------- | validation | 1.680 | 0.473 | 11220 |
| 04660 | training | 1.671 | 0.476 | ---------- |
| -------- | validation | 1.680 | 0.475 | 11348 |
| 04670 | training | 1.674 | 0.476 | ---------- |
| -------- | validation | 1.658 | 0.481 | 11475 |
| 04680 | training | 1.666 | 0.479 | ---------- |
| -------- | validation | 1.685 | 0.471 | 11604 |
| 04690 | training | 1.669 | 0.479 | ---------- |
| -------- | validation | 1.669 | 0.471 | 11732 |
| 04700 | training | 1.671 | 0.478 | ---------- |
| -------- | validation | 1.667 | 0.469 | 11869 |
| 04710 | training | 1.679 | 0.475 | ---------- |
| -------- | validation | 1.650 | 0.487 | 12080 |
