# Training `Apr15_2125`

Started on `2024-04-15 21:25:39.633823`

**Description:** Add description here

Arguments:
- **`--name`**: Apr15_2125
- **`--test`**: False
- **`--config`**: <_io.TextIOWrapper name='.\\configs\\resnet.yaml' mode='r' encoding='cp1252'>
- **`--batchsize`**: 512
- **`--lr`**: 0.001
- **`--exponential_decay`**: 0.999
- **`--reg_l2`**: 0.0005
- **`--momentum`**: 0.9
- **`--model`**: ResNet
- **`--criterion`**: CrossEntropyLoss()
- **`--optimizer`**: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
)

## Model

Saved in `runs/Apr15_2125.pt`

```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ResNet                                   [512, 1794]               --
├─Conv2d: 1-1                            [512, 64, 8, 8]           6,976
├─Sequential: 1-2                        [512, 64, 8, 8]           --
│    └─SimpleSkipLayer: 2-1              [512, 64, 8, 8]           256
│    │    └─Conv2d: 3-1                  [512, 64, 8, 8]           36,928
│    │    └─Conv2d: 3-2                  [512, 64, 8, 8]           36,928
│    └─SimpleSkipLayer: 2-2              [512, 64, 8, 8]           256
│    │    └─Conv2d: 3-3                  [512, 64, 8, 8]           36,928
│    │    └─Conv2d: 3-4                  [512, 64, 8, 8]           36,928
│    └─SimpleSkipLayer: 2-3              [512, 64, 8, 8]           256
│    │    └─Conv2d: 3-5                  [512, 64, 8, 8]           36,928
│    │    └─Conv2d: 3-6                  [512, 64, 8, 8]           36,928
│    └─SimpleSkipLayer: 2-4              [512, 64, 8, 8]           256
│    │    └─Conv2d: 3-7                  [512, 64, 8, 8]           36,928
│    │    └─Conv2d: 3-8                  [512, 64, 8, 8]           36,928
│    └─SimpleSkipLayer: 2-5              [512, 64, 8, 8]           256
│    │    └─Conv2d: 3-9                  [512, 64, 8, 8]           36,928
│    │    └─Conv2d: 3-10                 [512, 64, 8, 8]           36,928
├─Linear: 1-3                            [512, 1794]               7,350,018
==========================================================================================
Total params: 7,727,554
Trainable params: 7,727,554
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 16.09
==========================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 191.90
Params size (MB): 30.91
Estimated Total Size (MB): 224.38
==========================================================================================
```

## Training

| Epoch | Train/Val | Loss | Acc | Time |
| - | - | - | - | - |
| 00000 | training | 7.618 | 0.000 | ---------- |
| -------- | validation | 7.575 | 0.001 | 11 |
| 00010 | training | 5.558 | 0.089 | ---------- |
| -------- | validation | 4.772 | 0.147 | 39 |
| 00020 | training | 4.080 | 0.179 | ---------- |
| -------- | validation | 3.582 | 0.199 | 68 |
| 00030 | training | 3.380 | 0.219 | ---------- |
| -------- | validation | 3.206 | 0.238 | 97 |
| 00040 | training | 3.098 | 0.241 | ---------- |
| -------- | validation | 2.962 | 0.250 | 126 |
| 00050 | training | 2.919 | 0.258 | ---------- |
| -------- | validation | 2.829 | 0.272 | 154 |
| 00060 | training | 2.812 | 0.270 | ---------- |
| -------- | validation | 2.779 | 0.272 | 184 |
| 00070 | training | 2.732 | 0.278 | ---------- |
| -------- | validation | 2.700 | 0.288 | 215 |
| 00080 | training | 2.670 | 0.287 | ---------- |
| -------- | validation | 2.636 | 0.298 | 245 |
| 00090 | training | 2.617 | 0.294 | ---------- |
| -------- | validation | 2.610 | 0.290 | 276 |
| 00100 | training | 2.581 | 0.298 | ---------- |
| -------- | validation | 2.547 | 0.303 | 306 |
| 00110 | training | 2.537 | 0.305 | ---------- |
| -------- | validation | 2.534 | 0.304 | 336 |
| 00120 | training | 2.513 | 0.309 | ---------- |
| -------- | validation | 2.493 | 0.314 | 366 |
| 00130 | training | 2.490 | 0.313 | ---------- |
| -------- | validation | 2.435 | 0.315 | 395 |
| 00140 | training | 2.462 | 0.318 | ---------- |
| -------- | validation | 2.429 | 0.329 | 424 |
| 00150 | training | 2.435 | 0.323 | ---------- |
| -------- | validation | 2.430 | 0.323 | 452 |
| 00160 | training | 2.421 | 0.324 | ---------- |
| -------- | validation | 2.402 | 0.329 | 482 |
| 00170 | training | 2.397 | 0.329 | ---------- |
| -------- | validation | 2.400 | 0.326 | 511 |
| 00180 | training | 2.383 | 0.331 | ---------- |
| -------- | validation | 2.391 | 0.325 | 540 |
| 00190 | training | 2.374 | 0.333 | ---------- |
| -------- | validation | 2.359 | 0.328 | 571 |
| 00200 | training | 2.349 | 0.337 | ---------- |
| -------- | validation | 2.345 | 0.334 | 602 |
| 00210 | training | 2.342 | 0.339 | ---------- |
| -------- | validation | 2.378 | 0.332 | 632 |
| 00220 | training | 2.327 | 0.340 | ---------- |
| -------- | validation | 2.308 | 0.351 | 663 |
| 00230 | training | 2.315 | 0.343 | ---------- |
| -------- | validation | 2.354 | 0.333 | 692 |
| 00240 | training | 2.307 | 0.345 | ---------- |
| -------- | validation | 2.310 | 0.338 | 723 |
| 00250 | training | 2.295 | 0.346 | ---------- |
| -------- | validation | 2.275 | 0.352 | 754 |
| 00260 | training | 2.289 | 0.348 | ---------- |
| -------- | validation | 2.313 | 0.351 | 783 |
| 00270 | training | 2.281 | 0.349 | ---------- |
| -------- | validation | 2.261 | 0.351 | 812 |
| 00280 | training | 2.265 | 0.353 | ---------- |
| -------- | validation | 2.279 | 0.345 | 841 |
| 00290 | training | 2.254 | 0.354 | ---------- |
| -------- | validation | 2.255 | 0.347 | 871 |
| 00300 | training | 2.252 | 0.354 | ---------- |
| -------- | validation | 2.258 | 0.359 | 902 |
| 00310 | training | 2.248 | 0.355 | ---------- |
| -------- | validation | 2.244 | 0.363 | 932 |
| 00320 | training | 2.239 | 0.358 | ---------- |
| -------- | validation | 2.236 | 0.354 | 963 |
| 00330 | training | 2.232 | 0.358 | ---------- |
| -------- | validation | 2.259 | 0.343 | 992 |
| 00340 | training | 2.224 | 0.359 | ---------- |
| -------- | validation | 2.203 | 0.362 | 1022 |
| 00350 | training | 2.220 | 0.361 | ---------- |
| -------- | validation | 2.199 | 0.365 | 1051 |
| 00360 | training | 2.208 | 0.364 | ---------- |
| -------- | validation | 2.208 | 0.358 | 1081 |
| 00370 | training | 2.204 | 0.364 | ---------- |
| -------- | validation | 2.186 | 0.364 | 1110 |
| 00380 | training | 2.203 | 0.364 | ---------- |
| -------- | validation | 2.214 | 0.352 | 1140 |
| 00390 | training | 2.192 | 0.367 | ---------- |
| -------- | validation | 2.180 | 0.361 | 1169 |
| 00400 | training | 2.195 | 0.365 | ---------- |
| -------- | validation | 2.126 | 0.384 | 1199 |
| 00410 | training | 2.178 | 0.368 | ---------- |
| -------- | validation | 2.195 | 0.365 | 1229 |
| 00420 | training | 2.178 | 0.369 | ---------- |
| -------- | validation | 2.172 | 0.367 | 1258 |
| 00430 | training | 2.173 | 0.370 | ---------- |
| -------- | validation | 2.175 | 0.359 | 1288 |
| 00440 | training | 2.167 | 0.370 | ---------- |
| -------- | validation | 2.203 | 0.364 | 1320 |
| 00450 | training | 2.160 | 0.373 | ---------- |
| -------- | validation | 2.130 | 0.370 | 1352 |
| 00460 | training | 2.158 | 0.372 | ---------- |
| -------- | validation | 2.130 | 0.378 | 1385 |
| 00470 | training | 2.151 | 0.373 | ---------- |
| -------- | validation | 2.123 | 0.368 | 1417 |
| 00480 | training | 2.149 | 0.375 | ---------- |
| -------- | validation | 2.131 | 0.384 | 1448 |
| 00490 | training | 2.147 | 0.374 | ---------- |
| -------- | validation | 2.124 | 0.384 | 1480 |
| 00500 | training | 2.140 | 0.376 | ---------- |
| -------- | validation | 2.166 | 0.368 | 1512 |
| 00510 | training | 2.134 | 0.378 | ---------- |
| -------- | validation | 2.099 | 0.378 | 1546 |
| 00520 | training | 2.125 | 0.380 | ---------- |
| -------- | validation | 2.168 | 0.368 | 1578 |
| 00530 | training | 2.124 | 0.379 | ---------- |
| -------- | validation | 2.152 | 0.377 | 1610 |
| 00540 | training | 2.124 | 0.379 | ---------- |
| -------- | validation | 2.136 | 0.376 | 1642 |
| 00550 | training | 2.126 | 0.379 | ---------- |
| -------- | validation | 2.127 | 0.384 | 1676 |
| 00560 | training | 2.115 | 0.381 | ---------- |
| -------- | validation | 2.095 | 0.387 | 1709 |
| 00570 | training | 2.120 | 0.380 | ---------- |
| -------- | validation | 2.126 | 0.388 | 1741 |
| 00580 | training | 2.109 | 0.382 | ---------- |
| -------- | validation | 2.121 | 0.382 | 1773 |
| 00590 | training | 2.104 | 0.383 | ---------- |
| -------- | validation | 2.104 | 0.378 | 1805 |
| 00600 | training | 2.107 | 0.382 | ---------- |
| -------- | validation | 2.085 | 0.393 | 1836 |
| 00610 | training | 2.101 | 0.384 | ---------- |
| -------- | validation | 2.128 | 0.385 | 1868 |
| 00620 | training | 2.101 | 0.384 | ---------- |
| -------- | validation | 2.092 | 0.386 | 1900 |
| 00630 | training | 2.096 | 0.384 | ---------- |
| -------- | validation | 2.116 | 0.396 | 1931 |
| 00640 | training | 2.093 | 0.385 | ---------- |
| -------- | validation | 2.091 | 0.387 | 1963 |
| 00650 | training | 2.095 | 0.385 | ---------- |
| -------- | validation | 2.076 | 0.395 | 1994 |
| 00660 | training | 2.086 | 0.387 | ---------- |
| -------- | validation | 2.106 | 0.382 | 2025 |
| 00670 | training | 2.082 | 0.388 | ---------- |
| -------- | validation | 2.022 | 0.403 | 2056 |
| 00680 | training | 2.084 | 0.388 | ---------- |
| -------- | validation | 2.101 | 0.381 | 2088 |
| 00690 | training | 2.084 | 0.387 | ---------- |
| -------- | validation | 2.122 | 0.374 | 2119 |
| 00700 | training | 2.074 | 0.390 | ---------- |
| -------- | validation | 2.076 | 0.386 | 2152 |
| 00710 | training | 2.074 | 0.390 | ---------- |
| -------- | validation | 2.074 | 0.385 | 2183 |
| 00720 | training | 2.079 | 0.389 | ---------- |
| -------- | validation | 2.087 | 0.384 | 2215 |
| 00730 | training | 2.076 | 0.389 | ---------- |
| -------- | validation | 2.119 | 0.381 | 2247 |
| 00740 | training | 2.069 | 0.389 | ---------- |
| -------- | validation | 2.079 | 0.381 | 2278 |
| 00750 | training | 2.068 | 0.389 | ---------- |
| -------- | validation | 2.052 | 0.396 | 2310 |
| 00760 | training | 2.068 | 0.391 | ---------- |
| -------- | validation | 2.101 | 0.391 | 2341 |
| 00770 | training | 2.068 | 0.389 | ---------- |
| -------- | validation | 2.054 | 0.396 | 2373 |
| 00780 | training | 2.071 | 0.389 | ---------- |
| -------- | validation | 2.018 | 0.409 | 2405 |
| 00790 | training | 2.059 | 0.392 | ---------- |
| -------- | validation | 2.085 | 0.383 | 2436 |
| 00800 | training | 2.061 | 0.392 | ---------- |
| -------- | validation | 2.055 | 0.387 | 2468 |
| 00810 | training | 2.056 | 0.393 | ---------- |
| -------- | validation | 2.031 | 0.406 | 2499 |
| 00820 | training | 2.056 | 0.393 | ---------- |
| -------- | validation | 2.035 | 0.402 | 2530 |
| 00830 | training | 2.061 | 0.392 | ---------- |
| -------- | validation | 2.078 | 0.392 | 2561 |
| 00840 | training | 2.050 | 0.394 | ---------- |
| -------- | validation | 2.063 | 0.389 | 2592 |
| 00850 | training | 2.049 | 0.394 | ---------- |
| -------- | validation | 2.051 | 0.387 | 2624 |
| 00860 | training | 2.053 | 0.394 | ---------- |
| -------- | validation | 2.058 | 0.393 | 2655 |
| 00870 | training | 2.041 | 0.396 | ---------- |
| -------- | validation | 2.049 | 0.397 | 2686 |
| 00880 | training | 2.044 | 0.396 | ---------- |
| -------- | validation | 2.069 | 0.396 | 2716 |
| 00890 | training | 2.047 | 0.395 | ---------- |
| -------- | validation | 2.072 | 0.391 | 2748 |
| 00900 | training | 2.049 | 0.394 | ---------- |
| -------- | validation | 2.028 | 0.405 | 2781 |
| 00910 | training | 2.051 | 0.393 | ---------- |
| -------- | validation | 2.026 | 0.394 | 2812 |
| 00920 | training | 2.041 | 0.396 | ---------- |
| -------- | validation | 2.023 | 0.400 | 2843 |
| 00930 | training | 2.043 | 0.395 | ---------- |
| -------- | validation | 2.056 | 0.395 | 2874 |
| 00940 | training | 2.041 | 0.396 | ---------- |
| -------- | validation | 2.035 | 0.388 | 2904 |
| 00950 | training | 2.039 | 0.396 | ---------- |
| -------- | validation | 2.050 | 0.401 | 2935 |
| 00960 | training | 2.040 | 0.397 | ---------- |
| -------- | validation | 2.025 | 0.401 | 2966 |
| 00970 | training | 2.038 | 0.395 | ---------- |
| -------- | validation | 2.053 | 0.392 | 2997 |
| 00980 | training | 2.035 | 0.398 | ---------- |
| -------- | validation | 2.042 | 0.394 | 3028 |
| 00990 | training | 2.031 | 0.397 | ---------- |
| -------- | validation | 2.062 | 0.379 | 3059 |
| 01000 | training | 2.035 | 0.397 | ---------- |
| -------- | validation | 2.016 | 0.407 | 3090 |
| 01010 | training | 2.032 | 0.398 | ---------- |
| -------- | validation | 2.057 | 0.400 | 3121 |
| 01020 | training | 2.027 | 0.399 | ---------- |
| -------- | validation | 2.009 | 0.404 | 3152 |
| 01030 | training | 2.028 | 0.399 | ---------- |
| -------- | validation | 2.006 | 0.404 | 3183 |
| 01040 | training | 2.022 | 0.400 | ---------- |
| -------- | validation | 2.043 | 0.391 | 3212 |
| 01050 | training | 2.025 | 0.399 | ---------- |
| -------- | validation | 2.016 | 0.401 | 3243 |
| 01060 | training | 2.025 | 0.399 | ---------- |
| -------- | validation | 2.017 | 0.406 | 3272 |
| 01070 | training | 2.013 | 0.402 | ---------- |
| -------- | validation | 1.994 | 0.399 | 3302 |
| 01080 | training | 2.010 | 0.402 | ---------- |
| -------- | validation | 2.037 | 0.396 | 3332 |
| 01090 | training | 2.014 | 0.402 | ---------- |
| -------- | validation | 2.068 | 0.387 | 3362 |
| 01100 | training | 2.013 | 0.402 | ---------- |
| -------- | validation | 2.000 | 0.401 | 3392 |
| 01110 | training | 2.018 | 0.400 | ---------- |
| -------- | validation | 1.980 | 0.413 | 3422 |
| 01120 | training | 2.014 | 0.401 | ---------- |
| -------- | validation | 2.023 | 0.402 | 3452 |
| 01130 | training | 2.013 | 0.402 | ---------- |
| -------- | validation | 1.952 | 0.413 | 3481 |
| 01140 | training | 2.011 | 0.402 | ---------- |
| -------- | validation | 2.015 | 0.400 | 3510 |
| 01150 | training | 2.013 | 0.401 | ---------- |
| -------- | validation | 1.995 | 0.391 | 3539 |
| 01160 | training | 2.008 | 0.402 | ---------- |
| -------- | validation | 2.025 | 0.408 | 3569 |
| 01170 | training | 2.013 | 0.400 | ---------- |
| -------- | validation | 2.021 | 0.400 | 3601 |
| 01180 | training | 2.005 | 0.403 | ---------- |
| -------- | validation | 1.992 | 0.401 | 3632 |
| 01190 | training | 2.000 | 0.404 | ---------- |
| -------- | validation | 2.031 | 0.397 | 3664 |
| 01200 | training | 2.002 | 0.404 | ---------- |
| -------- | validation | 2.042 | 0.401 | 3697 |
| 01210 | training | 1.999 | 0.404 | ---------- |
| -------- | validation | 2.037 | 0.394 | 3730 |
| 01220 | training | 2.006 | 0.403 | ---------- |
| -------- | validation | 2.032 | 0.402 | 3764 |
| 01230 | training | 1.995 | 0.405 | ---------- |
| -------- | validation | 1.957 | 0.414 | 3797 |
| 01240 | training | 2.000 | 0.403 | ---------- |
| -------- | validation | 2.017 | 0.408 | 3829 |
| 01250 | training | 2.005 | 0.402 | ---------- |
| -------- | validation | 2.005 | 0.401 | 3861 |
| 01260 | training | 1.995 | 0.405 | ---------- |
| -------- | validation | 2.011 | 0.404 | 3893 |
| 01270 | training | 2.000 | 0.403 | ---------- |
| -------- | validation | 2.017 | 0.399 | 3925 |
| 01280 | training | 1.999 | 0.404 | ---------- |
| -------- | validation | 2.021 | 0.399 | 3957 |
| 01290 | training | 1.999 | 0.404 | ---------- |
| -------- | validation | 2.004 | 0.395 | 3988 |
| 01300 | training | 2.000 | 0.404 | ---------- |
| -------- | validation | 2.002 | 0.401 | 4020 |
| 01310 | training | 1.989 | 0.406 | ---------- |
| -------- | validation | 2.023 | 0.397 | 4052 |
| 01320 | training | 1.997 | 0.404 | ---------- |
| -------- | validation | 1.994 | 0.408 | 4083 |
| 01330 | training | 1.993 | 0.405 | ---------- |
| -------- | validation | 2.046 | 0.394 | 4116 |
| 01340 | training | 1.995 | 0.404 | ---------- |
| -------- | validation | 1.985 | 0.405 | 4148 |
| 01350 | training | 1.987 | 0.407 | ---------- |
| -------- | validation | 2.006 | 0.398 | 4179 |
| 01360 | training | 1.991 | 0.405 | ---------- |
| -------- | validation | 1.986 | 0.410 | 4212 |
| 01370 | training | 1.992 | 0.406 | ---------- |
| -------- | validation | 2.000 | 0.395 | 4244 |
| 01380 | training | 1.985 | 0.407 | ---------- |
| -------- | validation | 1.997 | 0.405 | 4276 |
| 01390 | training | 1.989 | 0.406 | ---------- |
| -------- | validation | 1.998 | 0.411 | 4307 |
| 01400 | training | 1.987 | 0.407 | ---------- |
| -------- | validation | 1.996 | 0.406 | 4338 |
| 01410 | training | 1.986 | 0.407 | ---------- |
| -------- | validation | 1.976 | 0.401 | 4368 |
| 01420 | training | 1.987 | 0.406 | ---------- |
| -------- | validation | 1.963 | 0.409 | 4400 |
| 01430 | training | 1.987 | 0.406 | ---------- |
| -------- | validation | 1.972 | 0.414 | 4430 |
| 01440 | training | 1.982 | 0.407 | ---------- |
| -------- | validation | 1.944 | 0.419 | 4462 |
| 01450 | training | 1.985 | 0.406 | ---------- |
| -------- | validation | 2.031 | 0.396 | 4492 |
| 01460 | training | 1.986 | 0.406 | ---------- |
| -------- | validation | 2.015 | 0.397 | 4525 |
| 01470 | training | 1.979 | 0.408 | ---------- |
| -------- | validation | 1.961 | 0.403 | 4556 |
| 01480 | training | 1.984 | 0.407 | ---------- |
| -------- | validation | 1.999 | 0.407 | 4588 |
